{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aleksandr Belov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side \\\\\n",
    "    &= right-hand-side on line 1 \\\\\n",
    "    &= right-hand-side on line 2 \\\\\n",
    "    &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The level lines of the surface $L_{\\infty}(x) = \\|x\\|_{\\infty}$ , $x \\in \\mathbb{R}^2$ are squre which has sides parallel to the axes. \n",
    "\n",
    "The level lines of the surface $L_{1}(x) = \\|x\\|_{1}$ , $x \\in \\mathbb{R}^2$ are squre too, but it is rotated by $\\frac{\\pi}{4}$ and the sides for $L_{1}(x) = L_{\\infty}(x) = z$, where $z$ is fix number, are $\\sqrt{2}$ times smaller.\n",
    "\n",
    "Then, this regression problem will be equal to Lasso regression if we apply 2 linear operators in the feature space $X$.\n",
    "\n",
    "Firstly we should rotate points of X by $\\theta = \\frac{\\pi}{4}$:\n",
    "$$R = \n",
    "\\begin{pmatrix}\n",
    "cos(\\theta) & -sin(\\theta)\\\\\n",
    "sin(\\theta) & cos(\\theta) \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "cos(\\frac{\\pi}{4}) & -sin(\\frac{\\pi}{4})\\\\\n",
    "sin(\\frac{\\pi}{4}) & cos(\\frac{\\pi}{4}) \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\sqrt{2}}{2} & -\\frac{\\sqrt{2}}{2}\\\\\n",
    "\\frac{\\sqrt{2}}{2} &  \\frac{\\sqrt{2}}{2}\\\\\n",
    "\\end{pmatrix} = \\frac{\\sqrt{2}}{2}\n",
    "\\begin{pmatrix}\n",
    "1 & -1\\\\\n",
    "1 & 1\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Secondly we should scale points of X $\\frac{1}{\\sqrt{2}}$ times:\n",
    "$$S = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "All in all, required linear operator is \n",
    "$$A = RS = \\frac{\\sqrt{2}}{2}\n",
    "\\begin{pmatrix}\n",
    "1 & -1\\\\\n",
    "1 & 1\\\\\n",
    "\\end{pmatrix}\\frac{1}{\\sqrt{2}} \n",
    "\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\end{pmatrix} = \n",
    "\\frac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "1 & -1\\\\\n",
    "1 & 1\\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic classifier model:\n",
    "$$\\mathbb{P}(y|x) = p(y|x; w,b)$$\n",
    "Maximum likelihood estimation for $w, b$:\n",
    "$$\\prod_{i=1}^{n}p(y_i|x_i; w,b) \\rightarrow \\max_{w, b}$$\n",
    "Log-likelihood for $w, b$:\n",
    "$$\\mathcal{L}(w, b) = \\sum_{i=1}^{n}\\log{p(y_i|x_i; w,b)} \\rightarrow \\max_{w, b}$$\n",
    "We have:\n",
    "$$p(y|x; w,b) = \\Phi(x^Tw + b)$$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    0 < \\Phi(t) < 1 \\\\\n",
    "    & \\forall -\\infty<t<+\\infty\\\\\n",
    "    \\Phi(-\\infty)=0 \\\\\n",
    "    \\Phi(+\\infty)=1\n",
    "\\end{align}\n",
    "\n",
    "Then\n",
    "$$\\mathcal{L}(w, b) = \\sum_{i=1}^{n}\\log{\\Phi(x^Tw + b)} \\rightarrow \\max_{w, b}$$\n",
    "$$\\mathcal{L}(w, b) = \\sum_{i=1}^{n}\\log{\\Phi^{-1}(x^Tw + b)} \\rightarrow \\min_{w, b}$$\n",
    "\n",
    "We have a lineary separable dataset and let $\\left(w^*, b^*\\right)$ be a required separating hyperplane such that $y_i\\left(x_i^Tw^* + b^*\\right) > 0$ for all $i$. Let's consider hyperplane which is determined by the parameters $\\lambda\\left(w^*, b^*\\right)$.\n",
    "\n",
    "$$\\lim_{\\lambda \\to +\\infty} \\mathcal{L}(\\lambda w^*, \\lambda b^*) = \\lim_{\\lambda \\to +\\infty} \\sum_{i=1}^{n}\\log{\\Phi^{-1}(\\lambda x^Tw^* + \\lambda b^*)} = 0$$\n",
    "\n",
    "$\\forall w, b$ such that $\\|w\\|<\\infty$ and $|b|<\\infty$:\n",
    "$$L(x, y, w, b) = \\log{\\Phi^{-1}(x^Tw + b)} > 0$$\n",
    "\n",
    "Therefore minimum of $\\mathcal{L}(w, b)$ is not achievable and the optimization problem doesn't have optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let: \n",
    " - $p_{i,j} = p(x_i=1|y=C_j)$ be a probability that $x_i = 1$ under condition $y=C_j$;\n",
    " - $p_j = p(y=C_j)$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = \\underset{y=\\{C_1, \\ldots, C_K\\}}{\\operatorname{argmax}} p(y)p(x|y) =\n",
    "\\underset{y=\\{C_1, \\ldots, C_K\\}}{\\operatorname{argmax}} p(y) \\prod_{i=1}^n p(x_i|y)=\n",
    "\\underset{j=\\{1, \\ldots, K\\}}{\\operatorname{argmax}} p_j \\prod_{i=1}^n p_{i, j}^{x_i}(1-p_{i, j})^{1-x_i}=$$ \n",
    "\n",
    "$$= \\underset{j=\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\bigg[\\log p_j + \\sum_{i=1}^n \\big(x_i\\log p_{i, j} + (1 - x_i)  \\log (1 - p_{i, j}) \\big)\\bigg]=$$ \n",
    "\n",
    "$$= \\underset{j=\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\bigg[\\log p_j + \\sum_{i=1}^n \\big(x_i \\log \\frac{p_{i, j}}{1-p_{i, j}} + \\log (1-p_{i,j}) \\big)\\bigg]=$$\n",
    "\n",
    "$$= \\underset{j=\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\bigg[\\big(\\log p_j + \\sum_{i=1}^{n}\\log (1-p_{i,j})\\big) + \\sum_{i=1}^n x_i \\log \\frac{p_{i, j}}{1-p_{i, j}}\\bigg] = $$\n",
    "\n",
    "$$ = \\underset{j=\\{1, \\ldots, K\\}}{\\operatorname{argmax}} \\bigg[\\big(b_j + \\sum_{i=1}^n x_iw_{i, j}\\bigg]$$\n",
    "\n",
    "Where:\n",
    " - $b_j$ is characterized by $C_j$;\n",
    " - $w_{i, j}$ is characterized by $x_i$ and $C_j$.\n",
    " \n",
    "So, decision rule of a Bayesian Naive Classifier is $\\arg \\max$ of linear functions of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X^k_j$ be denoted as a valid set and $X^n \\setminus X^k_j$ as a train set, where $|X^k_j|=k$, $|X^n \\setminus X^k_j|=n-k$ and $j$ is a splitting index.\n",
    "\n",
    "$$L_{k}OCV=\n",
    "\\frac{1}{C_{n}^{k}}\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg) = \n",
    "\\frac{1}{C_{n}^{k}}\\sum_{i=1}^{n}\\frac{1}{k}\\bigg(\\sum\\limits_{j=1}^{C_{n}^{k}}[x_i \\in X_j^k][y_{i}\\neq f_{X^{n}\\setminus X^k_j}( x_{i})]\\bigg) = (*)$$\n",
    "\n",
    "Let's consider number of errors at a fixed $m$:\n",
    " - The error will occur if and only if the first $m-1$ neighbors of sample $x_i$ are included in the valid set, and the $m$-th neighbor is included in the train set.\n",
    "Let's consider train and valied sets at a fixed $m$ and with a guaranteed error:\n",
    " - Size of valid set is $k$, the $m$-th neighbor of $x_i$ in the train set, the first $m-1$ neighbors in the valid set;\n",
    " - We have to take $n-k-1$ more samples to train set;\n",
    " - We have to choose from the remaining $n-m-1$ samples in the valid set;\n",
    " - Number of ways to select from $n-m-1$ elements $n-k-1$ elements is $C_{n-m-1}^{n-k-1}$.\n",
    " \n",
    "Then, number of errors at a fixed $m$ for $x_i$:\n",
    "$$[y_{i}\\neq y_{i}^{m}] C_{n-m-1}^{n-k-1}$$\n",
    " \n",
    "$$(*) = \\frac{1}{C_{n}^{k}}\\sum\\limits_{i=1}^n \\frac{1}{k}\\sum\\limits_{m=1}^k [y_{i}\\neq y_{i}^{m}] C_{n-m-1}^{n-k-1} = \n",
    "\\bigg[kC_{n}^{k} = \\frac{kn!}{k! (n-k)!} = \\frac{n(n-1)!}{(k-1)! (n-1-(k+1))!} = nC_{n-1}^{k-1}\\bigg] =\n",
    "\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bootstrap we should take each samples from $X^n$ uniformly with the probability $\\frac{1}{n}$. We should do it $n$ times and return the sample back to $X^n$. \n",
    "- The probability that $x_i$ wasn't taken on the $j$-th step is equal to $1 - \\frac{1}{n}$. \n",
    "- The probability that $x_i$ wasn't taken at all is equal to $\\left(1 - \\frac{1}{n}\\right)^n$.\n",
    "\n",
    "Let's find: $$\\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right)^n$$\n",
    "This limit is very similar to the one famous limit: $$\\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n}\\right)^n = e$$\n",
    "Let's lead the limit to the view:\n",
    "$$\\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right)^n = \\lim_{n \\to \\infty} \\left(\\left(1 - \\frac{1}{n}\\right)^{-n}\\right)^{-1}$$\n",
    "And use the limit above:\n",
    "$$\\lim_{n \\to \\infty} \\left(\\left(1 - \\frac{1}{n}\\right)^{-n}\\right)^{-1} = \\lim_{n \\to \\infty} e^{-1} = e^{-1}$$\n",
    "\n",
    "Then, the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$:\n",
    "$$\\lim_{n \\to \\infty} \\left(1 - \\frac{1}{n}\\right)^{n}= e^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will minimize the next functional:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}L(y_i, \\hat{y}) \\rightarrow min$$\n",
    "\n",
    "### 1. \n",
    "\n",
    "### 2.\n",
    "We want to minimize:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n} \\frac{(y_i-\\hat{y})^{2}}{y_i^2} \\rightarrow min$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\hat{y}} \\frac{1}{n}\\sum_{i=1}^{n} \\frac{(y_i-\\hat{y})^{2}}{y_i^2} = \n",
    "\\frac{2}{n}\\sum_{i=1}^{n} \\frac{\\hat{y}-y_i}{y_i^2} = \\frac{2}{n}\\bigg(\\hat{y}\\sum_{i=1}^{n} \\frac{1}{y_i^2} - \\sum_{i=1}^{n} \\frac{1}{y_i}\\bigg) = 0$$\n",
    "\n",
    "$$\\hat{y}\\sum_{i=1}^{n} \\frac{1}{y_i^2} - \\sum_{i=1}^{n} \\frac{1}{y_i}= 0$$\n",
    "\n",
    "$$\\hat{y}\\sum_{i=1}^{n} \\frac{1}{y_i^2} = \\sum_{i=1}^{n} \\frac{1}{y_i}$$\n",
    "\n",
    "$$\\hat{y}_{opt} = \\bigg(\\sum_{i=1}^{n} \\frac{1}{y_i}\\bigg) \\bigg( \\sum_{i=1}^{n} \\frac{1}{y_i^2}\\bigg)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $x, y \\in \\mathbb{R}^n$.\n",
    "\n",
    "## 1. \n",
    "### a)\n",
    "$$K(x,y)=\\langle x, y \\rangle ^ d = \\bigg(\\sum_{i=1}^{n}x_iy_i\\bigg)^d = \\underbrace{\\bigg(\\sum_{i=1}^{n}x_iy_i\\bigg) \\cdots\\bigg(\\sum_{i=1}^{n}x_iy_i\\bigg)}_{d -times} = \\sum_{(i_1, \\cdots, i_d) \\\\ 1\\leq i_k \\leq n}^{n}  \\prod_{k=1}^d x_{i_k}y_{i_k} = (*)$$\n",
    "\n",
    "Let's note that the total degree of each term of the last sum is equal to $d$. Then the sum looks like this:\n",
    "\n",
    "$$(*) = \\sum_{\\overline{a} = (a_1, \\cdots, a_n) \\\\ a_1 + \\cdots + a_n = d \\\\ 0 \\leq a_k \\leq d} C(a_1, \\cdots, a_n) \\prod_{k=1}^n x_k^{a_k}y_{k}^{a_k},$$\n",
    "\n",
    "where $C(a_1, \\cdots, a_n)$ is a coefficient that depends on $(a_1, \\cdots, a_n)$. \n",
    "\n",
    "### b)\n",
    "Let's find $C(a_1, \\cdots, a_n)$ which is the total number of term $\\prod_{k=1}^n x_k^{a_k}y_{k}^{a_k}$ in the sum for a fixed vector $\\overline{a} = (a_1, \\cdots, a_n) $.\n",
    " - We will not consider the order, then:\n",
    " $$C(a_1, \\cdots, a_n) = \\frac{d!}{a_1!\\cdots a_n!}$$\n",
    " \n",
    "### c)\n",
    "Let's count the total number of vectors $\\overline{a} = (a_1, \\cdots, a_n) $:\n",
    " - Let's create a binary vector of length $n + d -1$;\n",
    " - This vector has $n$ ones and $d-1$ zeros;\n",
    " - Number of ones before the $i$-th zero is equal to the degree of $i$-th term;\n",
    " - The number of these binary vectors is equal to $C_{n+d-1}^{d}$.\n",
    " \n",
    "Then\n",
    "$$\\varphi(x) \\in \\mathbb{R}^L,  L = C_{n+d-1}^{d}.$$\n",
    "\n",
    "### finally\n",
    "$$K(x,y) = \\sum_{\\overline{a} = (a_1, \\cdots, a_n)}\\frac{d!}{a_1!\\cdots a_n!} \\prod_{k=1}^n x_k^{a_k}y_{k}^{a_k}$$\n",
    "\n",
    "$$\\varphi(x) = \\begin{pmatrix}\n",
    "    \\sqrt{\\frac{d!}{a_1^{(1)}!\\cdots a_n^{(1)}!}} \\prod_{k=1}^n x_k^{a^{(1)}_k}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\sqrt{\\frac{d!}{a_1^{(L)}!\\cdots a_n^{(L)}!}} \\prod_{k=1}^n x_k^{a^{(L)}_k}\\\\\n",
    "  \\end{pmatrix}$$\n",
    "  \n",
    "  \n",
    "## 2.\n",
    "We introduce new vectors $\\tilde{x} = \\big(\\sqrt{c}, x_1, \\cdots, x_n\\big)$ and $\\tilde{y} = \\big(\\sqrt{c}, y_1, \\cdots, y_n\\big)$, then\n",
    "\n",
    "$$K(x,y)=\\big(\\langle x, y \\rangle + c \\big)^ d =  \\langle \\tilde{x}, \\tilde{y} \\rangle ^ d $$\n",
    "\n",
    "Using previous task:\n",
    "$$\\psi(\\tilde{x}) = \\begin{pmatrix}\n",
    "    \\sqrt{\\frac{d!}{b_1^{(1)}!\\cdots b_{n+1}^{(1)}!}} \\sqrt{c^{b^{(1)}_0}} \\prod_{k=1}^{n} x_k^{b^{(1)}_k}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\sqrt{\\frac{d!}{b_1^{(T)}!\\cdots b_{n+1}^{(T)}!}} \\sqrt{c^{b^{(T)}_0}} \\prod_{k=1}^{n} x_k^{b^{(T)}_k}\\\\\n",
    "  \\end{pmatrix}$$\n",
    "\n",
    "Where $$\\psi(\\tilde{x}) \\in \\mathbb{R}^T,  T = C_{n+d}^{d}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove it for L2-norm:\n",
    "\n",
    "Property of the positive definite Kernel function which we will use:\n",
    " - Linear combination of Kernels is a Kernel;\n",
    " - Product of Kernels is a Kernel;\n",
    " - Exponent of a Kernel is a Kernel *(from Canvas discussion https://skoltech.instructure.com/courses/2361/discussion_topics/10275)*;\n",
    " \n",
    "Let's rewrite this:\n",
    "$$\\|x-x'\\|^2 = (x-x')^T(x-x') = x^Tx - x^Tx' - x'^Tx + x'^Tx' = \\|x\\|^2 - 2x^Tx' + \\|x'\\|^2$$\n",
    "then\n",
    "$$exp(\\|x-x'\\|^2) = exp(\\|x\\|^2)exp(- 2x^Tx' )exp( \\|x'\\|^2)$$\n",
    "\n",
    "The Linear Kernel $x^\\top x$ is a positive definite. The exponent of the Linear Kernel also positive definite, then product of the exponents of the Linear Kernel is the positive definite Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
